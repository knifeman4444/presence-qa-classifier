model_name: "unsloth/gemma-3-270m-it"
model_short_name: "gemma-3"
output_dir: "data/models"
max_seq_length: 1024
load_in_4bit: false
load_in_8bit: false
full_finetuning: false

# LoRA configuration
lora:
  r: 8
  alpha: 16
  dropout: 0
  bias: "none"

# Training process
training:
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 8
  warmup_steps: 8
  max_steps: 1000
  learning_rate: 1e-3
  lr_scheduler_type: "linear"
  optim: "adamw_torch"
  logging_steps: 1

validation:
  per_device_eval_batch_size: 1
  eval_steps: 5
  save_steps: 5